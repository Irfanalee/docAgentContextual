# Interactive Contextual Retrieval System - User Guide

## ğŸ¯ Overview

`main.py` is an interactive command-line application that lets you ask questions about any PDF document using state-of-the-art contextual retrieval with hybrid search.

## ğŸš€ Quick Start

### Basic Usage

```bash
python main.py path/to/your/document.pdf
```

### With Real Context (Recommended for Production)

```bash
python main.py path/to/your/document.pdf --real-context
```

## ğŸ“‹ Features

### 1. **Automatic Document Processing**
- Loads PDF and extracts text
- Chunks document intelligently (800 tokens, 200 overlap)
- Generates contextual descriptions for each chunk
- Creates dual embeddings (standard + contextual)
- Builds both vector (Qdrant) and BM25 indexes

### 2. **Hybrid Search**
- Combines semantic search (vector embeddings)
- With lexical search (BM25 keyword matching)
- Automatically ranks results by relevance

### 3. **Interactive Q&A Session**
- Ask unlimited questions about your document
- Get top 3 most relevant results
- See scoring breakdown (vector + BM25)
- View context and chunk text

## ğŸ® How It Works

### Step-by-Step Process

```
1. Document Loading
   â””â”€> Reads PDF and extracts text

2. Chunking
   â””â”€> Splits into 800-token chunks with 200-token overlap

3. Context Generation
   â”œâ”€> Mock Mode: Fast, uses template context
   â””â”€> Real Mode: Uses Claude API for intelligent context

4. Embedding Generation
   â”œâ”€> Standard embedding (chunk only)
   â””â”€> Contextual embedding (context + chunk)

5. Dual Indexing
   â”œâ”€> Vector Store (Qdrant) for semantic search
   â””â”€> BM25 Index for keyword search

6. Interactive Session
   â””â”€> Ask questions â†’ Hybrid search â†’ Ranked results
```

## ğŸ’¡ Usage Examples

### Example 1: Quick Test (Mock Context)

```bash
# Fast processing for testing
python main.py data/reference.docx
```

**Output:**
```
======================================================================
ğŸ” CONTEXTUAL RETRIEVAL SYSTEM
======================================================================
Powered by: Contextual Embeddings + Hybrid Search
======================================================================

ğŸ“„ Loading document: data/reference.docx
âœ… Loaded 15234 characters

ğŸ“¦ Chunking document (size=800, overlap=200)...
âœ… Created 5 chunks

ğŸ§  Adding context to chunks...
   Using MOCK context for speed (use --real-context for Claude API)
âœ… Added context to all 5 chunks

ğŸ¯ Generating embeddings...
âœ… Generated dual embeddings for 5 chunks

ğŸ’¾ Storing in vector database (Qdrant)...
âœ… Stored in Qdrant with dual vectors

ğŸ“‡ Building BM25 index...
âœ… Built BM25 index

ğŸ”— Initializing hybrid retriever...
âœ… Hybrid retriever ready (50% vector + 50% BM25)

======================================================================
âœ… DOCUMENT LOADED AND INDEXED SUCCESSFULLY!
======================================================================

======================================================================
ğŸ’¡ INTERACTIVE SESSION STARTED
======================================================================
Ask questions about your document!
Commands:
  - Type your question and press Enter
  - 'quit' or 'exit' to end session
  - 'help' for more options
======================================================================

ğŸ” Your question:
```

### Example 2: Production Mode (Real Context)

```bash
# Best quality results using Claude API
python main.py data/reference.docx --real-context
```

**Benefits:**
- Claude generates intelligent, document-specific context
- Much better retrieval accuracy
- Takes longer to process (uses API calls)

### Example 3: Interactive Session

```
ğŸ” Your question: What is machine learning?

ğŸ” Searching...

ğŸ“Š Found 3 results:

======================================================================
Result #1
======================================================================
ğŸ“ˆ Combined Score: 0.8954
   â”œâ”€ Vector Score:  0.9234 (semantic similarity)
   â””â”€ BM25 Score:    0.8674 (keyword matching)

ğŸ’¬ Context:
   This chunk discusses machine learning as a subset of artificial
   intelligence, focusing on algorithms and data processing.

ğŸ“ Text:
   Machine learning is a branch of artificial intelligence that
   enables computers to learn from data without being explicitly
   programmed. It uses algorithms to identify patterns...

======================================================================
Result #2
======================================================================
ğŸ“ˆ Combined Score: 0.7821
   â”œâ”€ Vector Score:  0.8123 (semantic similarity)
   â””â”€ BM25 Score:    0.7519 (keyword matching)

ğŸ’¬ Context:
   This section explains different types of machine learning
   approaches including supervised and unsupervised learning.

ğŸ“ Text:
   There are three main types of machine learning: supervised
   learning, unsupervised learning, and reinforcement learning...

ğŸ“„ See full text of results? (y/n): n

ğŸ” Your question: stats

ğŸ“Š Document Statistics:
   Total chunks: 5
   Total characters: 15234
   Average chunk size: 3046 chars

ğŸ” Your question: quit

ğŸ‘‹ Goodbye! Thanks for using Contextual Retrieval System.
```

## ğŸ›ï¸ Command Options

### During Interactive Session

| Command | Description |
|---------|-------------|
| `<question>` | Ask any question about the document |
| `stats` | Show document statistics (chunks, characters, etc.) |
| `help` | Display available commands |
| `quit` / `exit` / `q` | End the session |

### Command Line Options

| Option | Description |
|--------|-------------|
| `<pdf-path>` | **Required**. Path to your PDF document |
| `--real-context` | Use Claude API for context generation (recommended for production) |

## ğŸ“Š Understanding Results

### Result Components

```
======================================================================
Result #1
======================================================================
ğŸ“ˆ Combined Score: 0.8954          â† Overall relevance (0-1 scale)
   â”œâ”€ Vector Score:  0.9234        â† Semantic similarity score
   â””â”€ BM25 Score:    0.8674        â† Keyword matching score

ğŸ’¬ Context:                         â† AI-generated context description
   This chunk discusses...

ğŸ“ Text:                            â† Actual chunk content
   Machine learning is...
```

### Score Interpretation

- **Combined Score**: Weighted average (50% vector + 50% BM25)
  - 0.9 - 1.0: Excellent match
  - 0.7 - 0.9: Good match
  - 0.5 - 0.7: Moderate match
  - < 0.5: Weak match

- **Vector Score**: How semantically similar the chunk is to your question
  - High: Chunk discusses similar concepts
  - Uses contextual embeddings

- **BM25 Score**: How well keywords match
  - High: Chunk contains exact query terms
  - Uses lexical matching

## ğŸ”§ Configuration

### Modify Search Behavior

Edit the code in `main.py`:

```python
# Change weights (line ~82)
hybrid_retriever = HybridRetriever(
    vector_store=storage,
    bm25_index=bm25_index,
    embedder=embedder,
    vector_weight=0.7,  # More weight on semantic search
    bm25_weight=0.3     # Less weight on keywords
)

# Change number of results (line ~163)
results = hybrid_retriever.retrieve(query, top_k=5)  # Get 5 results instead of 3

# Change chunk size (in config.py)
chunk_size = 1000          # Larger chunks
chunk_overlap = 250        # More overlap
```

## ğŸ› Troubleshooting

### Common Issues

**1. "Module not found" error**
```bash
# Solution: Install dependencies
pip install -r requirements.txt
```

**2. "Qdrant connection error"**
```bash
# Solution: Start Qdrant
docker run -p 6333:6333 qdrant/qdrant
```

**3. "File not found" error**
```bash
# Solution: Use correct path
python main.py data/reference.docx  # Correct
python main.py reference.docx       # Wrong (missing data/ prefix)
```

**4. "Claude API error" (with --real-context)**
```bash
# Solution: Check your API key in .env file
ANTHROPIC_API_KEY=your-key-here
```

## ğŸ“ Tips for Best Results

### 1. **Question Formulation**
```
âœ… Good: "What are the main benefits of machine learning?"
âœ… Good: "Explain neural networks"
âœ… Good: "How does gradient descent work?"

âŒ Avoid: Single words like "ML" or "AI"
âŒ Avoid: Too vague like "Tell me everything"
```

### 2. **Mock vs Real Context**

**Use Mock Context When:**
- Testing the system
- Quick iterations
- Document structure matters more than semantics

**Use Real Context When:**
- Production use
- Maximum accuracy needed
- Document has complex topics

### 3. **Document Types**

**Works Best With:**
- Technical documentation
- Research papers
- Reports and articles
- Educational materials

**May Need Tuning For:**
- Very short documents (< 1000 words)
- Heavily formatted documents
- Documents with lots of tables/figures

## ğŸ“ˆ Performance

### Processing Time

| Document Size | Mock Context | Real Context |
|---------------|--------------|--------------|
| 5 pages | ~10 seconds | ~30 seconds |
| 20 pages | ~30 seconds | ~2 minutes |
| 50 pages | ~1 minute | ~5 minutes |

**Note**: Real context time depends on Claude API response time

### Memory Usage

- Small doc (5 pages): ~100MB RAM
- Medium doc (20 pages): ~300MB RAM
- Large doc (50 pages): ~700MB RAM

## ğŸš€ Advanced Usage

### Batch Processing Multiple Documents

```bash
# Process multiple PDFs
for pdf in data/*.pdf; do
    echo "Processing $pdf"
    python main.py "$pdf" --real-context
done
```

### Integration with Scripts

```python
# Use as a library
from main import load_and_process_document

chunks, embedder, storage, bm25, retriever = load_and_process_document(
    "data/mydoc.pdf",
    use_mock_context=False
)

# Query programmatically
results = retriever.retrieve("your question", top_k=5)
for result in results:
    print(f"Score: {result['combined_score']}")
    print(f"Text: {result['chunk_text']}")
```

## ğŸ“š Architecture

### System Components

```
main.py
   â”‚
   â”œâ”€> Document Loader (src/document_loader.py)
   â”œâ”€> Chunker (src/chunker.py)
   â”œâ”€> Contextualizer (src/contextualizer.py)
   â”œâ”€> Embedder (src/embedder.py)
   â”œâ”€> Vector Store (src/vector_store.py)
   â”œâ”€> BM25 Index (src/bm25_index.py)
   â””â”€> Hybrid Retriever (src/retriever.py)
```

### Data Flow

```
PDF Input
   â†“
Text Extraction
   â†“
Chunking (800 tokens)
   â†“
Context Generation (Claude API or Mock)
   â†“
Dual Embeddings (Standard + Contextual)
   â†“
   â”œâ”€> Qdrant (Vector Search)
   â””â”€> BM25 (Keyword Search)
   â†“
Hybrid Retriever (Merge Results)
   â†“
Interactive Q&A
```

## ğŸ¯ Use Cases

### 1. **Research Assistant**
- Load research papers
- Ask specific questions about methodology
- Find relevant sections quickly

### 2. **Document Q&A**
- Company policies and procedures
- Technical documentation
- Training materials

### 3. **Study Aid**
- Load textbook chapters
- Ask questions to test understanding
- Get relevant explanations

### 4. **Content Discovery**
- Explore large documents
- Find specific information
- Understand document structure

## ğŸ“ Example Workflows

### Workflow 1: Academic Research

```bash
# 1. Load research paper
python main.py research/paper.pdf --real-context

# 2. Ask about methodology
ğŸ” Your question: What methodology was used in this study?

# 3. Find specific results
ğŸ” Your question: What were the main findings?

# 4. Understand implications
ğŸ” Your question: What are the practical applications?
```

### Workflow 2: Technical Documentation

```bash
# 1. Load API documentation
python main.py docs/api-reference.pdf

# 2. Quick queries (mock context is fine)
ğŸ” Your question: How do I authenticate?

# 3. Find examples
ğŸ” Your question: Show me an example of error handling

# 4. Get statistics
ğŸ” Your question: stats
```

## ğŸ”— Related Files

- `README.md` - Main project documentation
- `config.py` - Configuration settings
- `requirements.txt` - Python dependencies
- `.env` - API keys and secrets
- `tests/test_end_to_end.py` - Full pipeline test

## ğŸ“ Support

For issues or questions:
1. Check this guide first
2. Review `README.md` for system architecture
3. Run tests: `python tests/test_end_to_end.py`
4. Check GitHub issues

## ğŸ‰ Success Indicators

You know it's working when:
- âœ… Document loads without errors
- âœ… Search returns relevant results
- âœ… Scores make sense (higher for better matches)
- âœ… Results contain expected information
- âœ… Both vector and BM25 scores are > 0

---

**Happy Searching! ğŸš€**

*Built with Anthropic's Contextual Retrieval + Hybrid Search*
